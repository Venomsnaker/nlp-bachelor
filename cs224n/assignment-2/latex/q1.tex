\titledquestion{Understanding word2vec}[15]
Recall that the key insight behind {\tt word2vec} is that \textit{`a word is known by the company it keeps'}. Concretely, consider a `center' word $c$ surrounded before and after by a context of a certain length. We term words in this contextual window `outside words' ($O$). For example, in Figure~\ref{fig:word2vec}, the context window length is 2, the center word $c$ is `banking', and the outside words are `turning', `into', `crises', and `as':

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{word2vec.png}
    \caption{The word2vec skip-gram prediction model with window size 2}
    \label{fig:word2vec}
\end{figure}

Skip-gram {\tt word2vec} aims to learn the probability distribution $P(O|C)$. 
Specifically, given a specific word $o$ and a specific word $c$, we want to predict $P(O=o|C=c)$: the probability that word $o$ is an `outside' word for $c$ (i.e., that it falls within the contextual window of $c$).
We model this probability by taking the softmax function over a series of vector dot-products: % I added the word "softmax" here because I bet a lot of students will have forgotten what softmax is and why the loss fn is called naive softmax. but if this is too wordy we can just take it out

\begin{equation}
 P(O=o \mid C=c) = \frac{\exp(\bu_{o}^\top \bv_c)}{\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)}
 \label{word2vec_condprob}
\end{equation}

For each word, we learn vectors $u$ and $v$, where $\bu_o$ is the `outside' vector representing outside word $o$, and $\bv_c$ is the `center' vector representing center word $c$. 
We store these parameters in two matrices, $\bU$ and $\bV$.
The columns of $\bU$ are all the `outside' vectors $\bu_{w}$;
the columns of $\bV$ are all of the `center' vectors $\bv_{w}$. 
Both $\bU$ and $\bV$ contain a vector for every $w \in \text{Vocabulary}$.\footnote{Assume that every word in our vocabulary is matched to an integer number $k$. Bolded lowercase letters represent vectors. $\bu_{k}$ is both the $k^{th}$ column of $\bU$ and the `outside' word vector for the word indexed by $k$. $\bv_k$ is both the $k^{th}$ column of $\bV$ and the `center' word vector for the word indexed by $k$. \textbf{In order to simplify notation we shall interchangeably use $k$ to refer to word $k$ and the index of word $k$.}}\newline

%We can think of the probability distribution $P(O|C)$ as a prediction function that we can approximate via supervised learning. For any training example, we will have a single $o$ and $c$. We will then compute a value $P(O=o|C=c)$ and report the loss. 
Recall from lectures that, for a single pair of words $c$ and $o$, the loss is given by:

\begin{equation} 
\bJ_{\text{naive-softmax}}(\bv_c, o, \bU) = -\log P(O=o| C=c).
\label{naive-softmax}
\end{equation}
We can view this loss as the cross-entropy\footnote{The \textbf{cross-entropy loss} between the true (discrete) probability distribution $p$ and another distribution $q$ is $-\sum_i p_i \log(q_i)$.} between the true distribution $\by$ and the predicted distribution $\hat{\by}$, for a particular center word c and a particular outside word o. 
Here, both $\by$ and $\hat{\by}$ are vectors with length equal to the number of words in the vocabulary.
Furthermore, the $k^{th}$ entry in these vectors indicates the conditional probability of the $k^{th}$ word being an `outside word' for the given $c$. 
The true empirical distribution $\by$ is a one-hot vector with a 1 for the true outside word $o$, and 0 everywhere else, for this particular example of center word c and outside word o.\footnote{Note that the true conditional probability distribution of context words for the entire training dataset would not be one-hot.}
The predicted distribution $\hat{\by}$ is the probability distribution $P(O|C=c)$ given by our model in equation (\ref{word2vec_condprob}). \newline

\textbf{Note:} Throughout this homework, when computing derivatives, please use the method reviewed during the lecture (i.e. no Taylor Series Approximations).

\clearpage
\begin{parts}

\part[2]
Prove that the naive-softmax loss (Equation \ref{naive-softmax}) is the same as the cross-entropy loss between $\by$  and $\hat{\by}$, i.e. (note that $\by$ 
 (true distribution), $\hat{\by}$ (predicted distribution) are vectors and $\hat{\by}_o$ is a scalar):

\begin{equation} 
-\sum_{w \in \text{Vocab}} \by_w \log(\hat{\by}_w) = - \log (\hat{\by}_o).
\end{equation}

\ifans{
    \[
        y_w = 
        \begin{cases}
            1 & w = o\\
            0 & w \neq o
        \end{cases}
    \]    
    \( - \sum_{w \in Vocab} y_w log(\hat{y}_w) = -log(\hat{y}_o) - \sum_{w \in Vocab, \; w \neq o} {\sum} 0 (\hat{y}_w) = -log(\hat{y}_o) \)
}
% Question 1-B
\part[6]
\begin{enumerate}[label=\roman*.]
    \item 
    Compute the partial derivative of $\bJ_{\text{naive-softmax}}(\bv_c, o, \bU)$ with respect to $\bv_c$. \emph{Please write your answer in terms of $\by$, $\hat{\by}$, $\bU$, and show your work to receive full credit}.
    \begin{itemize} 
        \item \textbf{Note}: Your final answers for the partial derivative should follow the shape convention: the partial derivative of any function $f(x)$ with respect to $x$ should have the \textbf{same shape} as $x$.\footnote{This allows us to efficiently minimize a function using gradient descent without worrying about reshaping or dimension mismatching. While following the shape convention, we're guaranteed that $\theta:= \theta - \alpha\frac{\partial J(\theta)}{\partial \theta}$ is a well-defined update rule.}
        \item Please provide your answers for the partial derivative in vectorized form. For example, when we ask you to write your answers in terms of $\by$, $\hat{\by}$, and $\bU$, you may not refer to specific elements of these terms in your final answer (such as $\by_1$, $\by_2$, $\dots$). 
    \end{itemize}
    \item
    When is the gradient you computed equal to zero? \\
    \textbf{Hint:} You may wish to review and use some introductory linear algebra concepts.
    \item
    The gradient you found is the difference between the two terms. Provide an interpretation of how each of these terms improves the word vector when this gradient is subtracted from the word vector $v_c$.

\end{enumerate}

\ifans{
    \newline QUESTION I: \( \dfrac{\partial J_{naive-softmax}(v_c, o, U)}{\partial v_c}
    = \dfrac{\partial}{\partial v_c} [-log \dfrac{exp(u_o^T v_c)}{\sum_{w \in Vocab}exp(u_w^T v_c)}] \)
    \newline \( = \dfrac{\partial}{\partial v_c} [-log(exp(u_o^T v_c)) + log(\sum_{w \in Vocab}exp(u_w^T v_c))] \)
    \newline \( = \dfrac{\partial}{\partial v_c} [log(\sum_{w \in Vocab}exp(u_w^T v_c)) - u_o^T v_c] \)
    \newline \( = \sum_{x \in Vocab} \dfrac{exp(u_x^T v_c) u_w }{\sum_{w \in Vocab} exp(u_w^T v_c)} - u_o\)
    \newline \( = \sum_{x \in Vocab} P(O = x|C = c)u_w - u_o \)
    \newline $ = \E{[u_w]} - u_o $
    \newline \( = U^T(\hat{y} - y) \)
    \newline QUESTION II. \( \hat{y} = y \) (The model can make perfect predictions on outside words o from center word c)
    \newline QUESTION III. Subtracting this gradient from \(v_c\) helps minimize the loss function. (negative partial derivative values point toward minima)
}

% Question 1-C
\part[1]
In many downstream applications using word embeddings, L2 normalized vectors (e.g. $\mathbf{u}/||\mathbf{u}||_2$ where $||\mathbf{u}||_2 = \sqrt{\sum_i u_i^2}$) are used instead of their raw forms (e.g. $\mathbf{u}$). Letâ€™s consider a hypothetical downstream task of binary classification of phrases as being positive or negative, where you decide the sign based on the sum of individual embeddings of the words. When would L2 normalization take away useful information for the downstream task? When would it not?
\textbf{Hint:} Consider the case where $\mathbf{u}_x = \alpha\mathbf{u}_y$ for some words $x \neq y$ and some scalar $\alpha$. When $\alpha$ is positive, what will be the value of normalized  $\mathbf{u}_x$ and normalized $\mathbf{u}_y$? How might $\mathbf{u}_x$ and $\mathbf{u}_y$ be related for such a normalization to affect or not affect the resulting classification?

\ifans{
    \newline In the case of a word2vec model, L2 normalization will not affect the performance of a model with large word vector dimension much.
    \newline Meanwhile, models with smaller word vector dimensios will be affected heavily. Since this usage of L2 normalized vectors will smooth out severely U.  
}

% Question 1-D
\part[5] 
Compute the partial derivatives of $\bJ_{\text{naive-softmax}}(\bv_c, o, \bU)$ with respect to each of the `outside' word vectors, $\bu_w$'s. There will be two cases: when $w=o$, the true `outside' word vector, and $w \neq o$, for all other words. Please write your answer in terms of $\by$, $\hat{\by}$, and $\bv_c$. In this subpart, you may use specific elements within these terms as well (such as $\by_1$, $\by_2$, $\dots$). Note that $\bu_w$ is a vector while $\by_1, \by_2, \dots$ are scalars. Show your work to receive full credit.

\ifans{
    \newline $ \dfrac{\partial J_{naive-softmax}(v_c, o, U)}{\partial u_w} = \dfrac{\partial -log \dfrac{exp(u_o^T v_c)}{\sum_{w \in Vocab} exp(u_w^T v_c)}}{\partial u_w} $
    \newline $ = -\dfrac{\partial log(exp(u_o^T v_c))}{\partial u_w} +\dfrac{\partial log \sum_{w \in Vocab} exp(u_w^T v_c)}{\partial u_w} $
    \newline If $ w = 0 $:
    \newline $ = -v_c + \dfrac{exp(u^T_w v_c) v_c}{\sum_{w \in Vocab} exp(u_w^T v_c)} = (P(O = o| C = c) - 1)v_c $
    \newline If $ w \neq 0 $:
    \newline $ = 0 + \dfrac{exp(u^T_w v_c) v_c}{\sum_{w \in Vocab} exp(u_w^T v_c)} = P(O = o| C = c) v_c $
    \newline So: $ \dfrac{\partial J_{naive-softmax}(v_c, o, U)}{\partial u_w} = (\hat{y} - y)^T v_c $    
}

% Question 1-E
\part[1]
Write down the partial derivative of $\bJ_{\text{naive-softmax}}(\bv_c, o, \bU)$ with respect to $\bU$. Please break down your answer in terms of the column vectors $\frac{\partial \bJ(\bv_c, o, \bU)}{\partial \bu_1}$, $\frac{\partial \bJ(\bv_c, o, \bU)}{\partial \bu_2}$, $\cdots$, $\frac{\partial \bJ(\bv_c, o, \bU)}{\partial \bu_{|\text{Vocab}|}}$. No derivations are necessary, just an answer in the form of a matrix.

\ifans {
    \newline \( \dfrac{\partial J_{naive-softmax}(v_c, o, U)}{\partial u_w} = (\hat{y} - y)^T v_c \)
    \newline Therefore: $ \dfrac{\partial J_{naive-softmax}(v_c, o, U)}{\partial U} $
    \newline \( = \begin{bmatrix} \hat{y}_1 v_c & \hat{y}_2 v_c & ... & \hat{y}_o v_c - v_c & ... & \hat{y}_{|Vocab|} v_c\end{bmatrix} \)


    
}


\end{parts}