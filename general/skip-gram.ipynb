{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate skip-grams\n",
    "skip_grams = [tf.keras.preprocessing.sequence.skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in word_ids]\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id_to_word[pairs[i][0]], pairs[i][0], \n",
    "          id_to_word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate skip-grams\n",
    "skip_grams = [tf.keras.preprocessing.sequence.skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in word_ids]\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id_to_word[pairs[i][0]], pairs[i][0], \n",
    "          id_to_word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Dense, Embedding, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the input layers for the target and context words\n",
    "target_word_input = tf.keras.Input(shape=(1,))\n",
    "context_word_input = tf.keras.Input(shape=(1,))\n",
    "\n",
    "# Build skip-gram architecture\n",
    "target_word_model = Embedding(vocab_size, embedding_size,\n",
    "                              embeddings_initializer=\"glorot_uniform\")(target_word_input)\n",
    "target_word_model = Reshape((embedding_size,))(target_word_model)\n",
    "\n",
    "context_word_model = Embedding(vocab_size, embedding_size,\n",
    "                               embeddings_initializer=\"glorot_uniform\")(context_word_input)\n",
    "context_word_model = Reshape((embedding_size,))(context_word_model)\n",
    "\n",
    "# Concatenate the output of the target and context models\n",
    "merged = Concatenate(axis=1)([target_word_model, context_word_model])\n",
    "\n",
    "# Add a dense layer and sigmoid activation\n",
    "output = Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\")(merged)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[target_word_input, context_word_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "# View model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on the skip-grams\n",
    "for epoch in range(1, 6):\n",
    "    total_loss = 0\n",
    "    for i, elem in enumerate(skip_grams):\n",
    "        skip_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        skip_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [skip_first_elem, skip_second_elem]\n",
    "        Y = labels\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed {} skip-gram pairs'.format(i))\n",
    "        total_loss += model.train_on_batch(X,Y)  \n",
    "\n",
    "    print('Epoch: {} Loss: {}'.format(epoch, total_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# get the embeddings for the words in the vocabulary\n",
    "weights = model.layers[2].get_weights()[0]\n",
    "\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n",
    "\n",
    "print(similar_words)\n",
    "\n",
    "# reduce the dimensions of the embeddings using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "vectors_2d = tsne.fit_transform(weights)\n",
    "\n",
    "# create a list of the words in the vocabulary\n",
    "words = [id2word[i] for i in range(1, vocab_size)]\n",
    "\n",
    "# plot the similar words\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "for word in similar_words:\n",
    "    ax.scatter(vectors_2d[word2id[word]-1, 0], vectors_2d[word2id[word]-1, 1], c='red', label=word)\n",
    "    for sim_word in similar_words[word]:\n",
    "        ax.scatter(vectors_2d[word2id[sim_word]-1, 0], vectors_2d[word2id[sim_word]-1, 1], c='blue')\n",
    "        ax.annotate(sim_word, (vectors_2d[word2id[sim_word]-1, 0], vectors_2d[word2id[sim_word]-1, 1]))\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
